from numpy import *
import numpy as np
import pandas as pd
import os
import sqlite3 as sql
os.system('python -m venv venv && venv\\Scripts\\activate.bat && pip install pipreqs && pipreqs "' + os.getcwd() +'" && pip install -r requirements.txt')
def sqlConnect():
    try:
        conn = sql.connect(':memory:')
        return conn
    except:
        print('Connection could not be setup')


def ATM_Call_Put_Spread(dailyStockFile, optionMetricsCRSPLinkFile, optionPriceFolder, outputFile):
    # Bali and Hovakimian (2009) shows that the spread between the implied volatilities of
    # at-the-money (ATM) call options and put options predicts the future returns of the underying stock.
    # This code works with the end-of-day data provided by OptionMetrics.
    # OptionMetrics provides the best bids and offers within the day for each option contract at the end of each day.
    # It also provides the following information: implied volatility, option contract volume, open interest, etc.
    # At the end of each month, Bali and Hovakimian (2009) select ATM options satisfying the following conditions:
    #   1) expire in 30-90 days
    #   2) have open interest >0 ; best bid > 0
    #   3) 0.9 <= moneyness <= 1.1 where moneyness is S/K for calls and K/S for puts
    #   4) quoted bid ask spread percentage <= 0.5
    # At the end of each month, if there are multiple contracts per firm,
    # then take the average implied volatilities of these multiple contracts

    # INPUTS:
    # dailyStockFile: is the CRSP CSV file that has the daily stock price data needed to calculate the moneyness
    # optionMetricsCRSPLinkFile: is the CSV file that links the OptionMetrics identifiers (i.e, SECID) to
    #       CRSP identifiers (i.e, PERMNO) with scoring. It is generated by WRDS.
    # optionPriceFolder: is the full path of the folder that has the big OptionMetrics daily option data.
    #                   Since the daily files are too big, I split the full OptionMetrics sample into multiple files
    #                   under this folder.
    # OUTPUTS:
    # outputFile: is the CSV file where the final results are written to.
    conn = sqlConnect()
    dsf = pd.read_csv(dailyStockFile)
    dsf['year'] = floor(dsf['DATE'].values / 10000).astype(int)
    dsf['yymm'] = floor(dsf['DATE'].values / 100).astype(int)

    trading_days = dsf.loc[dsf.groupby('yymm')['DATE'].idxmax()]
    trading_days.to_sql('trading_days', conn)

    oclink = pd.read_csv(optionMetricsCRSPLinkFile)
    oclink.to_sql('oclink', conn)
    priceFiles = os.listdir(optionPriceFolder)
    for file in priceFiles:
        year_text = file.replace('.csv', '').split('-')
        start_year = int(year_text[0])
        finish_year = int(year_text[1]) + 1
        years = range(start_year, finish_year)
        for year in years:
            allDF = pd.DataFrame()
            dsf2 = dsf[dsf['year'] == year]
            dsf2.to_sql('dsf', conn, if_exists='replace')
            chunks = pd.read_csv(optionPriceFolder + file,
                                 usecols=['secid', 'optionid', 'date', 'exdate', 'cp_flag', 'best_bid', 'best_offer',
                                          'volume',
                                          'strike_price', 'open_interest', 'impl_volatility'], chunksize=10000000)
            for chunk in chunks:
                chunk['year'] = floor(chunk['date'].values / 10000).astype(int)
                chunk = chunk[chunk['year'] == year]
                chunk['yyyymm'] = floor(chunk['date'].values / 100).astype(int)
                if not chunk.empty:
                    chunk['days_to_maturity'] = (pd.to_datetime(chunk['exdate'], errors='coerce', format='%Y%m%d') - pd.to_datetime(chunk['date'], errors='coerce', format='%Y%m%d')).dt.days
                    chunk = chunk[(chunk['days_to_maturity'] >= 30) & (chunk['days_to_maturity'] <= 90)]
                    chunk = chunk[(chunk['best_bid'] > 0) & (chunk['open_interest'] > 0)]
                    chunk['price'] = (chunk['best_bid'] + chunk['best_offer']) / 2
                    chunk['bid_ask_spread'] = (chunk['best_bid'] - chunk['best_offer']).abs()
                    chunk = chunk[(chunk['bid_ask_spread'] / chunk['price']) <= 0.5]
                    chunk = chunk[chunk['impl_volatility'].notnull()]

                    chunk.to_sql('chunk', conn, if_exists='replace')
                    query = '''
                            select distinct permno,secid,optionid,cp_flag,yyyymm,date,moneyness,impl_volatility
                                from
                                (
                                select distinct a.*,b.secid,b.permno,abs(c.prc) as prc ,abs(c.prc)/(a.strike_price/1000) as moneyness
                                    from chunk as a, oclink as b, dsf as c
                                where
                                    a.secid=b.secid
                                    and b.score in (0,1,2,3)
                                    and b.permno=c.permno
                                    and a.date=c.date
                                    and a.year=''' + str(year) + ''')'''
                    chunk = pd.read_sql_query(query, conn)
                    chunk = chunk[(chunk['moneyness'] >= exp(-0.1)) & (chunk['moneyness'] <= exp(0.1))]
                    allDF = allDF.append(chunk)
                else:
                    continue
            if not allDF.empty:
                allDF.to_sql('to_summarize', conn, if_exists='replace')
                query = '''select distinct permno,secid,optionid,cp_flag,yyyymm,date,moneyness,impl_volatility
                from to_summarize
                group by permno,secid,optionid,yyyymm
                having
                max(date)=date'''

                allDF = pd.read_sql_query(query, conn)
                allDF.to_sql('to_summarize', conn, if_exists='replace')
                query = '''
                    select distinct a.permno,a.yyyymm,call.cvol,put.pvol,call.cvol-put.pvol as cvol_pvol,avg_vol.ivol
                        from
                        (
                        select distinct permno,yyyymm
                            from 
                        to_summarize
                        ) as a
                        left join
                        (
                            select distinct permno,yyyymm,avg(impl_volatility) as cvol
                                from 
                                to_summarize
                                where cp_flag='C'
                                group by permno,yyyymm
                        ) as call
                        on
                        a.permno=call.permno
                        and a.yyyymm=call.yyyymm
                        left join 
                        (
                            select distinct permno,yyyymm,avg(impl_volatility) as ivol
                            from
                            to_summarize
                            group by permno,yyyymm

                        ) as avg_vol
                        on
                        a.permno=avg_vol.permno
                        and a.yyyymm=avg_vol.yyyymm
                        left join
                        (
                            select distinct permno,yyyymm,avg(impl_volatility) as pvol
                                from 
                                to_summarize
                                where cp_flag='P'
                                group by permno,yyyymm
                        ) as put
                         on
                         a.permno=put.permno
                         and a.yyyymm=put.yyyymm'''
                allDF = pd.read_sql_query(query, conn)
                allDF.to_csv(outputFile, index=False, mode='a', header=False)

