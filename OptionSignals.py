from numpy import *
import numpy as np
import pandas as pd
import os
import sqlite3 as sql
os.system('python -m venv venv && venv\\Scripts\\activate.bat && pip install pipreqs && pipreqs "' + os.getcwd() +'" && pip install -r requirements.txt')
def sqlConnect():
    try:
        conn = sql.connect(':memory:')
        return conn
    except:
        print('Connection could not be setup')


def ATM_Call_Put_Spread(dailyStockFile, optionMetricsCRSPLinkFile, optionPriceFolder, outputFile):
    # Bali and Hovakimian (2009) shows that the spread between the implied volatilities of
    # at-the-money (ATM) call options and put options predicts the future returns of the underying stock.
    # This code works with the end-of-day data provided by OptionMetrics.
    # OptionMetrics provides the best bids and offers within the day for each option contract at the end of each day.
    # It also provides the following information: implied volatility, option contract volume, open interest, etc.
    # At the end of each month, Bali and Hovakimian (2009) select ATM options satisfying the following conditions:
    #   1) expire in 30-90 days
    #   2) have open interest >0 ; best bid > 0
    #   3) 0.9 <= moneyness <= 1.1 where moneyness is S/K for calls and K/S for puts
    #   4) quoted bid ask spread percentage <= 0.5
    # At the end of each month, if there are multiple contracts per firm,
    # then take the average implied volatilities of these multiple contracts

    # INPUTS:
    # dailyStockFile: is the CRSP CSV file that has the daily stock price data needed to calculate the moneyness
    # optionMetricsCRSPLinkFile: is the CSV file that links the OptionMetrics identifiers (i.e, SECID) to
    #       CRSP identifiers (i.e, PERMNO) with scoring. It is generated by WRDS.
    # optionPriceFolder: is the full path of the folder that has the big OptionMetrics daily option data.
    #                   Since the daily files are too big, I split the full OptionMetrics sample into multiple files
    #                   under this folder.
    # OUTPUTS:
    # outputFile: is the CSV file where the final results are written to.
    conn = sqlConnect()
    dsf = pd.read_csv(dailyStockFile)
    dsf['year'] = floor(dsf['DATE'].values / 10000).astype(int)
    dsf['yymm'] = floor(dsf['DATE'].values / 100).astype(int)

    trading_days = dsf.loc[dsf.groupby('yymm')['DATE'].idxmax()]
    trading_days.to_sql('trading_days', conn)

    oclink = pd.read_csv(optionMetricsCRSPLinkFile)
    oclink.to_sql('oclink', conn)
    priceFiles = os.listdir(optionPriceFolder)
    for file in priceFiles:
        year_text = file.replace('.csv', '').split('-')
        start_year = int(year_text[0])
        finish_year = int(year_text[1]) + 1
        years = range(start_year, finish_year)
        for year in years:
            allDF = pd.DataFrame()
            dsf2 = dsf[dsf['year'] == year]
            dsf2.to_sql('dsf', conn, if_exists='replace')
            chunks = pd.read_csv(optionPriceFolder + file,
                                 usecols=['secid', 'optionid', 'date', 'exdate', 'cp_flag', 'best_bid', 'best_offer',
                                          'volume',
                                          'strike_price', 'open_interest', 'impl_volatility'], chunksize=10000000)
            for chunk in chunks:
                chunk['year'] = floor(chunk['date'].values / 10000).astype(int)
                chunk = chunk[chunk['year'] == year]
                chunk['yyyymm'] = floor(chunk['date'].values / 100).astype(int)
                if not chunk.empty:
                    chunk['days_to_maturity'] = (pd.to_datetime(chunk['exdate'], errors='coerce', format='%Y%m%d') - pd.to_datetime(chunk['date'], errors='coerce', format='%Y%m%d')).dt.days
                    chunk = chunk[(chunk['days_to_maturity'] >= 30) & (chunk['days_to_maturity'] <= 90)]
                    chunk = chunk[(chunk['best_bid'] > 0) & (chunk['open_interest'] > 0)]
                    chunk['price'] = (chunk['best_bid'] + chunk['best_offer']) / 2
                    chunk['bid_ask_spread'] = (chunk['best_bid'] - chunk['best_offer']).abs()
                    chunk = chunk[(chunk['bid_ask_spread'] / chunk['price']) <= 0.5]
                    chunk = chunk[chunk['impl_volatility'].notnull()]

                    chunk.to_sql('chunk', conn, if_exists='replace')
                    query = '''
                            select distinct permno,secid,optionid,cp_flag,yyyymm,date,moneyness,impl_volatility
                                from
                                (
                                select distinct a.*,b.secid,b.permno,abs(c.prc) as prc ,abs(c.prc)/(a.strike_price/1000) as moneyness
                                    from chunk as a, oclink as b, dsf as c
                                where
                                    a.secid=b.secid
                                    and b.score in (0,1,2,3)
                                    and b.permno=c.permno
                                    and a.date=c.date
                                    and a.year=''' + str(year) + ''')'''
                    chunk = pd.read_sql_query(query, conn)
                    chunk = chunk[(chunk['moneyness'] >= exp(-0.1)) & (chunk['moneyness'] <= exp(0.1))]
                    allDF = allDF.append(chunk)
                else:
                    continue
            if not allDF.empty:
                allDF.to_sql('to_summarize', conn, if_exists='replace')
                query = '''select distinct permno,secid,optionid,cp_flag,yyyymm,date,moneyness,impl_volatility
                from to_summarize
                group by permno,secid,optionid,yyyymm
                having
                max(date)=date'''

                allDF = pd.read_sql_query(query, conn)
                allDF.to_sql('to_summarize', conn, if_exists='replace')
                query = '''
                    select distinct a.permno,a.yyyymm,call.cvol,put.pvol,call.cvol-put.pvol as cvol_pvol,avg_vol.ivol
                        from
                        (
                        select distinct permno,yyyymm
                            from 
                        to_summarize
                        ) as a
                        left join
                        (
                            select distinct permno,yyyymm,avg(impl_volatility) as cvol
                                from 
                                to_summarize
                                where cp_flag='C'
                                group by permno,yyyymm
                        ) as call
                        on
                        a.permno=call.permno
                        and a.yyyymm=call.yyyymm
                        left join 
                        (
                            select distinct permno,yyyymm,avg(impl_volatility) as ivol
                            from
                            to_summarize
                            group by permno,yyyymm

                        ) as avg_vol
                        on
                        a.permno=avg_vol.permno
                        and a.yyyymm=avg_vol.yyyymm
                        left join
                        (
                            select distinct permno,yyyymm,avg(impl_volatility) as pvol
                                from 
                                to_summarize
                                where cp_flag='P'
                                group by permno,yyyymm
                        ) as put
                         on
                         a.permno=put.permno
                         and a.yyyymm=put.yyyymm'''
                allDF = pd.read_sql_query(query, conn)
                allDF.to_csv(outputFile, index=False, mode='a', header=False)


def OTM_Put_ATM_Call_Spread(dailyStockFile, optionMetricsCRSPLinkFile, optionPriceFolder):
    '''
    i) The underlying stock’s volume for that day is positive.
    ii) The underlying stock’s price for that day is higher than $5.
    iii) The implied volatility of the option is between 3% and 200%.
    iv) The option’s price (average of best bid price and best ask price) is higher than
    $0.125.
    v) The option contract has positive open interest and nonmissing volume data.
    vi) The option matures within 10–60 days.
    vii) A put option is defined as OTM when the ratio of the strike price to the stock price
    is lower than 0.95 (but higher than 0.80), and a call option is defined as ATM
    when the ratio of the strike price to the stock price is between 0.95 and 1.05
    vii) If there are multiple ATM call or OTM puts for each stock on a day, then two approaches:
            * select ATM calls with moneyness being closest to 1; select OTM puts with moneyness being closest to 0.95
            ** calculate volume-weighted implied volatities,
            *** or select the option contract with the highest trading volume or open interest
            **** or select the option contract which is the closest to being ATM call or being OTM put
            '''

    def writeHeader():
        with open(optionPriceFolder + 'options_date.csv', 'w') as f:
            f.write('''SECID,OPTIONID,PERMNO,DATE''' + '\n')
        with open(optionPriceFolder + 'otm-atm volatility spread (highest).csv', 'w') as f:
            f.write('''PERMNO,DATE,HVVOL_OTM,HVVOL_ATM,HVVS,HOVOL_OTM,HOVOL_ATM,HOVS''' + '\n')
        with open(optionPriceFolder + 'otm-atm volatility spread (weighted).csv', 'w') as f:
            f.write('''PERMNO,DATE,VWVS_ATM,VWVS_OTM,VWVS,OWVS_ATM,OWVS_OTM,OWVS''' + '\n')
        with open(optionPriceFolder + 'otm-atm volatility spread (closest).csv', 'w') as f:
            f.write('''PERMNO,DATE,MONEYNESS_VOL_ATM,MONEYNESS_VOL_OTM,MONEYNESS_VS''' + '\n')
        with open(optionPriceFolder + 'otm-atm volatility spread (all together).csv', 'w') as f:
            f.write('''PERMNO,DATE,VWVS_ATM,VWVS_OTM,VWVS,OWVS_ATM,OWVS_OTM,OWVS,HVVOL_OTM,HVVOL_ATM,HVVS,HOVOL_OTM,HOVOL_ATM,HOVS,MONEYNESS_VOL_ATM,MONEYNESS_VOL_OTM,MONEYNESS_VS''' + '\n')
    def closest(conn, fname=optionPriceFolder + 'otm-atm volatility spread (closest).csv'):
        atm_query = '''
            select distinct permno,date,impl_volatility
            from
                (
                select distinct permno,date,open_interest,impl_volatility
                from
                    (
                    select distinct permno,date,open_interest,volume,impl_volatility
                    from
                        to_summarize
                    where
                        cp_flag='C'
                    group by permno,date
                    having min(abs(impl_volatility-1))=abs(impl_volatility-1)
                    )
                    group by permno,date
                    having max(volume)=volume
                )
                group by permno,date
                having max(open_interest)=open_interest
                    '''
        pd.read_sql_query(atm_query, conn).to_sql('atm', conn, if_exists='replace')
        otm_query = '''
                select distinct permno,date,impl_volatility
                from
                    (
                    select distinct permno,date,open_interest,impl_volatility
                    from
                        (
                        select distinct permno,date,open_interest,volume,impl_volatility
                        from
                            to_summarize
                        where
                            cp_flag='P'
                        group by permno,date
                        having min(abs(impl_volatility-0.95))=abs(impl_volatility-0.95)
                        )
                        group by permno,date
                        having max(volume)=volume
                    )
                    group by permno,date
                    having max(open_interest)=open_interest
                        '''
        pd.read_sql_query(otm_query, conn).to_sql('otm', conn, if_exists='replace')
        final_query = '''select distinct 
                            a.permno,a.date, 
                            b.impl_volatility as atm_ivol,
                            c.impl_volatility as otm_ivol,
                            c.impl_volatility-b.impl_volatility as closest_vol_spread
                            from
                            to_summarize as a
                            left join atm as b
                            on
                            a.permno=b.permno
                            and a.date=b.date
                            left join otm as c
                            on
                            a.permno=c.permno
                            and a.date=c.date'''
        chunks = pd.read_sql_query(final_query, conn, chunksize=10000000)
        for chunk in chunks:
            chunk.to_csv(fname, index=False, mode='a', header=False)

    def highest_ivol(conn, fname=optionPriceFolder + 'otm-atm volatility spread (highest).csv'):
        highest_vol_atm = '''
                    select distinct permno,date,impl_volatility
                        from  
                            (   
                            select distinct permno,date,open_interest,impl_volatility
                            from
                                to_summarize
                            where
                                cp_flag='C'
                                group by permno,date
                                having max(volume)=volume
                            )
                            group by permno,date
                            having max(open_interest)=open_interest'''
        pd.read_sql_query(highest_vol_atm, conn).to_sql('highest_vol_atm', conn, if_exists='replace')
        highest_vol_otm = '''    
                        select distinct permno,date,impl_volatility
                        from
                            ( 
                            select distinct permno,date,open_interest,impl_volatility
                            from
                                to_summarize
                            where
                                cp_flag='P'
                                group by permno,date
                                having max(volume)=volume
                            )
                            group by permno,date
                            having max(open_interest)=open_interest'''
        pd.read_sql_query(highest_vol_otm, conn).to_sql('highest_vol_otm', conn, if_exists='replace')
        highest_openint_otm = '''  
                        select distinct permno,date,impl_volatility
                        from
                            (
                            select distinct permno,date,volume,impl_volatility
                            from
                                to_summarize
                            where
                                cp_flag='P'
                                group by permno,date
                                having max(open_interest)=open_interest
                            )    
                            group by permno,date
                            having max(volume)=volume'''
        pd.read_sql_query(highest_openint_otm, conn).to_sql('highest_openint_OTM', conn, if_exists='replace')
        highest_openint_atm = '''
                        select distinct permno,date,impl_volatility
                            from
                            (
                            select distinct permno,date,volume,impl_volatility
                            from
                                to_summarize
                            where
                                cp_flag='C'
                                group by permno,date
                                having max(open_interest)=open_interest
                            )    
                            group by permno,date
                            having max(volume)=volume'''
        pd.read_sql_query(highest_openint_atm, conn).to_sql('highest_openint_ATM', conn, if_exists='replace')
        query = '''select distinct a.permno, a.date,
                            highest_vol_OTM.impl_volatility as otm_highest_vol_ivol,
                            highest_vol_ATM.impl_volatility as atm_highest_vol_ivol,
                            highest_vol_OTM.impl_volatility- highest_vol_ATM.impl_volatility as hvvs,
                            highest_openint_OTM.impl_volatility as otm_highest_oi_ivol,
                            highest_openint_ATM.impl_volatility as atm_highest_oi_ivol,
                            highest_openint_OTM.impl_volatility- highest_openint_ATM.impl_volatility as hovs
                            from
                            (select distinct permno,date,optionid from to_summarize) as a

                            left join highest_vol_ATM
                            on
                            a.permno=highest_vol_ATM.permno
                            and a.date=highest_vol_ATM.date

                            left join highest_vol_OTM
                            on
                            a.permno=highest_vol_OTM.permno
                            and a.date=highest_vol_OTM.date

                            left join highest_openint_ATM
                            on
                            a.permno=highest_openint_ATM.permno
                            and a.date=highest_openint_ATM.date

                            left join highest_openint_OTM
                            on
                            a.permno=highest_openint_OTM.permno
                            and a.date=highest_openint_OTM.date
                        '''
        chunks = pd.read_sql_query(query, conn, chunksize=10000000)
        for chunk in chunks:
            chunk.to_csv(fname, index=False, mode='a', header=False)

    def weighted_ivol(conn, fname=optionPriceFolder + 'otm-atm volatility spread (weighted).csv'):
        query = '''
                        select distinct permno,date,
                            sum(open_interest_wt_ivol/total_open_interest) as owvs_otm,
                            sum(vol_wt_ivol/total_open_interest) as vwvs_otm
                            from
                            (
                            select distinct a.permno,a.date,a.optionid,a.impl_volatility,a.open_interest,a.volume,
                                a.open_interest_wt_ivol,
                                a.vol_wt_ivol,
                                b.total_vol,b.total_open_interest
                                from
                                (select distinct a.permno,a.date,a.optionid,a.open_interest,a.volume,a.impl_volatility,
                                 a.open_interest*a.impl_volatility as open_interest_wt_ivol,
                                a.volume*a.impl_volatility as  vol_wt_ivol from to_summarize as a)
                                as a
                                left join (
                                select distinct permno,optionid,date,sum(volume) as total_vol,sum(open_interest) as total_open_interest
                                        from to_summarize
                                        where
                                        cp_flag='P'
                                        group by permno,date
                                ) as b
                                on
                                a.permno=b.permno
                                and a.date=b.date
                            )
                            group by permno,date
                                '''
        pd.read_sql_query(query, conn).to_sql('otm', conn, if_exists='replace')
        query = '''select distinct permno,date,
                            sum(open_interest_wt_ivol/total_open_interest) as owvs_atm,
                            sum(vol_wt_ivol/total_open_interest) as vwvs_atm
                            from
                            (
                            select distinct a.permno,a.date,a.optionid,a.impl_volatility,a.open_interest,a.volume,
                                a.open_interest_wt_ivol,
                                a.vol_wt_ivol,
                                b.total_vol,b.total_open_interest
                                from
                                (select distinct a.permno,a.date,a.optionid,a.open_interest,a.volume,a.impl_volatility,
                                 a.open_interest*a.impl_volatility as open_interest_wt_ivol,
                                a.volume*a.impl_volatility as  vol_wt_ivol from to_summarize as a)
                                as a
                                left join (
                                select distinct permno,optionid,date,sum(volume) as total_vol,sum(open_interest) as total_open_interest
                                        from to_summarize
                                        where
                                        cp_flag='C'
                                        group by permno,date
                                ) as b
                                on
                                a.permno=b.permno
                                and a.date=b.date
                            )
                            group by permno,date
                                '''
        pd.read_sql_query(query, conn).to_sql('atm', conn, if_exists='replace')
        vwvs_query = '''
                            select distinct a.permno,a.date, 
                            ATM.vwvs_atm, OTM.vwvs_otm,  OTM.vwvs_otm- ATM.vwvs_atm  as vwvs,
                            ATM.owvs_atm, OTM.owvs_otm,  OTM.owvs_otm- ATM.owvs_atm  as owvs
                            from
                            (select distinct permno,date from to_summarize) as a
                            left join
                            otm as OTM
                            on
                            a.permno=OTM.permno
                            and a.date=OTM.date
                            left join
                            atm as ATM
                            on
                            a.permno=ATM.permno
                            and a.date=ATM.date
                                    '''
        pd.read_sql_query(vwvs_query, conn).to_csv(fname, index=False, mode='a', header=False)

    def combine_measures(infname1=optionPriceFolder + 'options_date.csv',
                         infname2=optionPriceFolder + 'otm-atm volatility spread (weighted).csv',
                         infname3=optionPriceFolder + 'otm-atm volatility spread (highest).csv',
                         infname4=optionPriceFolder + 'otm-atm volatility spread (closest).csv',
                         outfname=optionPriceFolder + 'otm-atm volatility spread (all together).csv'):
        conn = sqlConnect()
        data = pd.read_csv(infname1).drop_duplicates()
        data.to_sql('options_date', conn)
        pd.read_csv(infname2).to_sql('weighted', conn)
        pd.read_csv(infname3).to_sql('highest', conn)
        pd.read_csv(infname4).to_sql('closest', conn)
        query = '''select distinct 
                            a.permno,a.date,
                            b.VWVS_ATM,b.VWVS_OTM,b.VWVS,
                            b.OWVS_ATM,b.OWVS_OTM,b.OWVS,
                            c.HVVOL_OTM,c.HVVOL_ATM,c.HVVS,c.HOVOL_OTM,c.HOVOL_ATM,c.HOVS,
                            d.MONEYNESS_VOL_ATM,d.MONEYNESS_VOL_OTM,d.MONEYNESS_VS
                            from 
                            (select distinct permno, date from options_date) as a
                            left join weighted as b
                            on
                            a.permno=b.permno
                            and a.date=b.date
                            left join highest as c
                            on
                            a.permno=c.permno
                            and a.date=c.date
                            left join closest as d
                            on
                            a.permno=d.permno
                            and a.date=d.date'''
        chunks = pd.read_sql_query(query, conn, chunksize=10000000)
        for chunk in chunks:
            chunk.to_csv(outfname, index=False, mode='a', header=False)
        k = 2
    writeHeader()
    dsf = pd.read_csv(dailyStockFile)
    dsf['year'] = floor(dsf['DATE'].values / 10000).astype(int)
    priceFiles = os.listdir(optionPriceFolder)
    conn = sqlConnect()
    pd.read_csv(optionMetricsCRSPLinkFile).to_sql('oclink', conn)
    for file in priceFiles:
        year_text = file.replace('.csv', '').split('-')
        start_year = int(year_text[0])
        finish_year = int(year_text[1]) + 1
        years = range(start_year, finish_year)
        for year in years:
            allDF = pd.DataFrame()
            dsf2 = dsf[dsf['year'] == year]
            dsf2.to_sql('dsf', conn, if_exists='replace')
            chunks = pd.read_csv(optionPriceFolder + file,
                                 usecols=['secid', 'optionid', 'date', 'exdate', 'cp_flag', 'best_bid', 'best_offer',
                                          'volume',
                                          'strike_price', 'open_interest', 'impl_volatility'], chunksize=15000000)

            for chunk_id, chunk in enumerate(chunks):
                chunk['year'] = floor(chunk['date'].values / 10000).astype(int)
                chunk = chunk[chunk['year'] == year]
                chunk['yyyymm'] = floor(chunk['date'].values / 100).astype(int)
                if not chunk.empty:
                    chunk['days_to_maturity'] = (pd.to_datetime(chunk['exdate'], errors='coerce', format='%Y%m%d') - pd.to_datetime(chunk['date'], errors='coerce', format='%Y%m%d')).dt.days
                    chunk = chunk[(chunk['days_to_maturity'] >= 10) & (chunk['days_to_maturity'] <= 60)]
                    chunk = chunk[(chunk['volume'].notnull()) & (chunk['open_interest'] > 0) & (chunk['impl_volatility'].notnull())]
                    chunk['price'] = (chunk['best_bid'] + chunk['best_offer']) / 2
                    chunk = chunk[chunk['price'] >= 0.125]
                    chunk = chunk[(chunk['impl_volatility'] >= 0.03) & chunk['impl_volatility'] <= 200]
                    chunk.to_sql('chunk', conn, if_exists='replace')
                    query = '''
                            select distinct permno,secid,optionid,cp_flag,yyyymm,date,moneyness,impl_volatility,volume,open_interest
                                from
                                (
                                select distinct a.*,b.secid,b.permno,abs(c.prc) as prc ,(a.strike_price/1000)/abs(c.prc) as moneyness
                                    from chunk as a, oclink as b, dsf as c
                                where
                                    a.secid=b.secid
                                    and b.score in (0,1,2,3)
                                    and b.permno=c.permno
                                    and a.date=c.date
                                    and a.year=''' + str(year) + ''')'''
                    chunk = pd.read_sql_query(query, conn)
                    chunk = chunk[
                        ((chunk['moneyness'] >= 0.8) & (chunk['moneyness'] < 0.95) & (chunk['cp_flag'] == 'P')) |
                        ((chunk['moneyness'] >= 0.95) & (chunk['moneyness'] <= 1.05) & (chunk['cp_flag'] == 'C'))]
                    allDF = allDF.append(chunk)
                else:
                    continue

            if not allDF.empty:
                allDF.to_sql('to_summarize', conn, if_exists='replace')
                allDF[['secid', 'optionid', 'permno', 'date']].drop_duplicates().to_csv(optionPriceFolder + 'options_date.csv', mode='a', header=False, index=False)
                weighted_ivol(conn)
                highest_ivol(conn)
                closest(conn)
    conn.close()
    combine_measures()


def Put_Call_Parity_Deviation(dailyStockFile, optionMetricsCRSPLinkFile, optionPriceFolder):
    projectPath = os.getcwd().replace('\\','/') + '/'
    oclink = pd.read_csv(optionMetricsCRSPLinkFile,usecols=['PERMNO','secid','SCORE'])
    oclink=oclink[oclink['SCORE'].isin([0,1,2,3])]
    oclink.columns=[col.upper() for col in oclink.columns]
    oclink=oclink[['PERMNO','SECID']]
    dsf=pd.read_csv(dailyStockFile)


    files=os.listdir(optionPriceFolder)
    for file in files:
        year_text=file.replace('.csv','').split('-')
        start_year=int(year_text[0])
        finish_year = int(year_text[1])+1
        years_list=list(np.arange(start_year,finish_year))
        headerFlag=True
        #Bring option related data from OptionMetrics
        chunks=pd.read_csv(optionPriceFolder+file,chunksize=10000000,usecols=['secid','optionid','date','exdate','strike_price','cp_flag','best_bid','best_offer','open_interest','volume','impl_volatility'])
        for chunk in chunks:
            chunk.columns = [col.upper() for col in chunk.columns]
            chunk = pd.merge(chunk,oclink,on=['SECID'])
            #finish_chunk=pd.merge(start,chunk2,left_on=['SECID','DATE_END'],right_on=['SECID','DATE'])
            #finish_chunk.rename(columns={'STRIKE_PRICE':'STRIKE'},inplace=True)
            if not chunk.empty:
                chunk = chunk[chunk['CP_FLAG'].notnull()]
                chunk['STRIKE'] = chunk['STRIKE_PRICE']/1000
                chunk['OPTION_PRICE'] = abs( (chunk['BEST_BID']+chunk['BEST_OFFER'])/2)
                chunk = chunk[(chunk['OPEN_INTEREST'] > 0) & (chunk['BEST_BID'] < chunk['BEST_OFFER']) & (chunk['BEST_BID'] > 0) ]
                chunk = chunk[chunk['OPTION_PRICE'] >= 0.125]
                chunk = chunk[(chunk['IMPL_VOLATILITY'] >= 0.03) &(chunk['IMPL_VOLATILITY'] <= 2)]
                chunk['DATE2'] = pd.to_datetime(chunk['DATE'],format='%Y%m%d',errors='coerce')
                chunk['EXDATE2'] = pd.to_datetime(chunk['EXDATE'],format='%Y%m%d',errors='coerce')
                chunk['T'] = (chunk['EXDATE2']-chunk['DATE2']).dt.days
                chunk = chunk[(chunk['T']>=10) &(chunk['T']<=60)]

                #Merge with CRSP
                chunk = pd.merge(chunk,dsf,on=['PERMNO','DATE'])
                chunk['MONEYNESS'] =chunk['STRIKE']/chunk['PRC']
                chunk = chunk[(chunk['MONEYNESS']<=1.1) & (chunk['MONEYNESS']>=0.90)]
                #Basic arbitrage conditions
                chunk.loc[chunk['CP_FLAG'] == 'C','INTRINSIC_VALUE'] = chunk['PRC']-chunk['STRIKE']
                chunk.loc[chunk['CP_FLAG'] == 'P','INTRINSIC_VALUE'] = chunk['STRIKE']-chunk['PRC']
                chunk.loc[(chunk['INTRINSIC_VALUE']<0),'INTRINSIC_VALUE']=0
                chunk = chunk[ ( ((chunk['CP_FLAG'] == 'C') & (chunk['PRC'] >= chunk['BEST_BID']) )
                                 | ((chunk['CP_FLAG'] == 'P')  & (chunk['STRIKE'] >= chunk['BEST_BID'])) )
                               & (chunk['BEST_OFFER'] >= chunk['INTRINSIC_VALUE'])].reset_index(drop=True)
                chunk.drop(columns=['DATE2','EXDATE2','STRIKE_PRICE','BEST_BID','BEST_OFFER','INTRINSIC_VALUE','MONEYNESS'],inplace=True)
                chunk['YEAR'] = (chunk['DATE']/10000).astype(int)
                years = np.sort(list(chunk['YEAR'].unique()))
                for year in years:
                    outFile = projectPath + str(year)+'.csv'
                    chunk2 = chunk[chunk['YEAR'] == year]
                    chunk2.drop(columns=['YEAR','T','OPTION_PRICE','PRC'],inplace=True)
                    if os.path.exists(outFile):
                        chunk2.to_csv(outFile,index=False,mode='a',header=False)
                    else:
                        chunk2.to_csv(outFile,index=False,mode='a',header=True)
        for year in years_list:
            inFile = projectPath+str(year)+'.csv'
            data = pd.read_csv(inFile)
            pairs = data.groupby(['PERMNO','SECID','DATE','EXDATE','STRIKE','CP_FLAG'])['OPTIONID'].count().reset_index()
            pairs = pairs.groupby(['PERMNO','SECID','DATE','EXDATE','STRIKE'])['OPTIONID'].sum().reset_index()
            pairs = pairs[pairs['OPTIONID']==2]
            data = pd.merge(data,pairs,on=['PERMNO','SECID','DATE','EXDATE','STRIKE'],suffixes=('','_y'))
            data['PAIR_ID'] = data.groupby(['PERMNO','SECID','DATE','EXDATE','STRIKE']).ngroup()
            calls = data[data['CP_FLAG']=='C']
            puts = data[data['CP_FLAG']=='P']
            calls = pd.merge(calls,puts,how='left',on=['PERMNO','SECID','DATE','EXDATE','STRIKE','PAIR_ID'],suffixes=('_call','_put'))
            calls['OPEN_INTEREST_avg'] = (calls['OPEN_INTEREST_call']+calls['OPEN_INTEREST_put'])/2
            total_openint = calls.groupby(['PERMNO','DATE'])['OPEN_INTEREST_avg'].sum().reset_index()
            calls = pd.merge(calls,total_openint,how='left',on=['PERMNO','DATE'],suffixes=('','_total'))
            calls['OIW_PARITY_SPREAD'] = (calls['IMPL_VOLATILITY_put']-calls['IMPL_VOLATILITY_call'])*calls['OPEN_INTEREST_avg']/calls['OPEN_INTEREST_avg_total']
            calls['EW_PARITY_SPREAD'] = (calls['IMPL_VOLATILITY_put']-calls['IMPL_VOLATILITY_call'])

            firm_date = calls[['PERMNO','DATE']].drop_duplicates()
            open_int_weighted = calls.groupby(['PERMNO','DATE'])['OIW_PARITY_SPREAD'].sum().reset_index()
            equal_weighted = calls.groupby(['PERMNO','DATE'])['EW_PARITY_SPREAD'].mean().reset_index()

            firm_date = pd.merge(firm_date, open_int_weighted, how='left', on = ['PERMNO','DATE'])
            firm_date = pd.merge(firm_date, equal_weighted, how='left', on = ['PERMNO','DATE'])
            firm_date.to_csv( projectPath + str(year)+'_put_call_parity_spread.csv',index=False)
